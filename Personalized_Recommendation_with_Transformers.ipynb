{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "In this blog post, we will define our transformer model and generate personalized recommendations based on user sequences at MovieLens dataset. From data pre-processing and model training to making the final predictions, we will go through all steps one by one."
      ],
      "metadata": {
        "id": "t5chOLwu7CEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSj6Zhto6v_y"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preprocessing\n",
        "In this section, we'll start by loading the MovieLens dataset. We will then construct vocabularies for movie IDs and user IDs, and create sequences of user interactions. These steps lay the groundwork for our recommendation model, converting the data into a format that our model can utilize effectively.\n",
        "## 1.1 Loading Dataset\n",
        "At first we will download our dataset to generate our sequences and vocabularies. Then user_id and movie_id values are processesed to fix their data types."
      ],
      "metadata": {
        "id": "ewsQzPX17M6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n",
        "ZipFile(\"movielens.zip\", \"r\").extractall()"
      ],
      "metadata": {
        "id": "4x3UJlOc7llA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Datasets\n",
        "users = pd.read_csv(\n",
        "    \"ml-1m/users.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
        ")\n",
        "\n",
        "ratings = pd.read_csv(\n",
        "    \"ml-1m/ratings.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
        ")\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"ml-1m/movies.dat\", sep=\"::\", names=[\"movie_id\", \"title\", \"genres\"], encoding='latin-1'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppvDGJzE7MtC",
        "outputId": "cb2b654a-7b14-4be3-ff62-f5daccdcce89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0a8b069686a2>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  users = pd.read_csv(\n",
            "<ipython-input-3-0a8b069686a2>:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  ratings = pd.read_csv(\n",
            "<ipython-input-3-0a8b069686a2>:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  movies = pd.read_csv(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preventing ids to be written as integer or float data type\n",
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")"
      ],
      "metadata": {
        "id": "eJoH9z2D7L3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Creating Vocabulary\n",
        "Now that we have our data ready, it's time to prepare our vocabularies for user IDs and movie IDs. This step will convert the unique IDs into numerical indices that our model can use. The following code snippet accomplishes this task."
      ],
      "metadata": {
        "id": "zE0CZxBN8IHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Genarting a list of unique movie ids\n",
        "movie_ids = movies.movie_id.unique()\n",
        "\n",
        "# Counter is used to feed movies to movive_vocab\n",
        "movie_counter = Counter(movie_ids)\n",
        "\n",
        "# Genarting vocabulary\n",
        "movie_vocab = vocab(movie_counter, specials=['<unk>'])\n",
        "\n",
        "# For indexing input ids\n",
        "movie_vocab_stoi = movie_vocab.get_stoi()\n",
        "\n",
        "# Movie to title mapping dictionary\n",
        "movie_title_dict = dict(zip(movies.movie_id, movies.title))\n",
        "\n",
        "# Similarly generating a vocabulary for user ids\n",
        "user_ids = users.user_id.unique()\n",
        "user_counter = Counter(user_ids)\n",
        "user_vocab = vocab(user_counter, specials=['<unk>'])\n",
        "user_vocab_stoi = user_vocab.get_stoi()"
      ],
      "metadata": {
        "id": "rcNNeirg8F2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Generating Sequences\n",
        "All interactions of users are first sorted by their interaction timestamp and then divided into sub sequences to train our model."
      ],
      "metadata": {
        "id": "Uxhd4YPy9WNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group ratings by user_id in order of increasing unix_timestamp.\n",
        "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
        "\n",
        "ratings_data = pd.DataFrame(\n",
        "    data={\n",
        "        \"user_id\": list(ratings_group.groups.keys()),\n",
        "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
        "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Sequence length and window slide size\n",
        "sequence_length = 4\n",
        "step_size = 2\n",
        "\n",
        "# Creating sequences from lists with sliding window\n",
        "def create_sequences(values, window_size, step_size):\n",
        "    sequences = []\n",
        "    start_index = 0\n",
        "    while True:\n",
        "        if start_index >= len(values):\n",
        "            break\n",
        "        end_index = start_index + window_size\n",
        "        seq = values[start_index:end_index]\n",
        "        if len(seq) < window_size:\n",
        "            seq = values[-window_size:]\n",
        "            if len(seq) == window_size:\n",
        "                sequences.append(seq)\n",
        "            break\n",
        "        sequences.append(seq)\n",
        "        start_index += step_size\n",
        "    return sequences\n",
        "\n",
        "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "del ratings_data[\"timestamps\"]\n",
        "\n",
        "# Sub-sequences are exploded.\n",
        "# Since there might be more than one sequence for each user.\n",
        "ratings_data_transformed = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
        "    \"movie_ids\", ignore_index=True\n",
        ")\n",
        "\n",
        "ratings_data_transformed.rename(\n",
        "    columns={\"movie_ids\": \"sequence_movie_ids\"},\n",
        "    inplace=True,\n",
        ")"
      ],
      "metadata": {
        "id": "d1OO4GqD8NX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data_transformed.sample(frac=1).reset_index(drop=True).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q21pt9ww9seT",
        "outputId": "3b7b5360-c6d5-4b13-a6dc-cb851754f36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     user_id                                sequence_movie_ids\n",
              "0  user_1500  [movie_2772, movie_2881, movie_2599, movie_3159]\n",
              "1  user_4028   [movie_1287, movie_2001, movie_172, movie_1370]\n",
              "2  user_4488    [movie_3638, movie_2405, movie_485, movie_380]\n",
              "3  user_3776  [movie_2001, movie_2746, movie_2406, movie_2100]\n",
              "4    user_82    [movie_1411, movie_1784, movie_62, movie_2268]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-68fb33d5-8c35-46a9-b655-c9716e5927e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>sequence_movie_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_1500</td>\n",
              "      <td>[movie_2772, movie_2881, movie_2599, movie_3159]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_4028</td>\n",
              "      <td>[movie_1287, movie_2001, movie_172, movie_1370]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_4488</td>\n",
              "      <td>[movie_3638, movie_2405, movie_485, movie_380]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_3776</td>\n",
              "      <td>[movie_2001, movie_2746, movie_2406, movie_2100]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_82</td>\n",
              "      <td>[movie_1411, movie_1784, movie_62, movie_2268]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68fb33d5-8c35-46a9-b655-c9716e5927e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-c1ad1e2b-7b34-4526-838a-516282403ca2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1ad1e2b-7b34-4526-838a-516282403ca2')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-c1ad1e2b-7b34-4526-838a-516282403ca2 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68fb33d5-8c35-46a9-b655-c9716e5927e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68fb33d5-8c35-46a9-b655-c9716e5927e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Train Test Split\n",
        "The data is split into training and testing sets. Although considering timestamps could potentially provide a more refined split, for the sake of simplicity, we opt for a random indexing approach."
      ],
      "metadata": {
        "id": "-0wqTDXi_1vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random indexing\n",
        "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
        "\n",
        "# Split train data\n",
        "df_train_data = ratings_data_transformed[random_selection]\n",
        "train_data_raw = df_train_data[[\"user_id\", \"sequence_movie_ids\"]].values\n",
        "\n",
        "# Split test data\n",
        "df_test_data = ratings_data_transformed[~random_selection]\n",
        "test_data_raw = df_test_data[[\"user_id\", \"sequence_movie_ids\"]].values"
      ],
      "metadata": {
        "id": "5WOLmSIz9tYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader is defined to be used for training and evaluation as final pre-processing step."
      ],
      "metadata": {
        "id": "XVKMFIwd_6vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch Dataset for user interactions\n",
        "class MovieSeqDataset(Dataset):\n",
        "    # Initialize dataset\n",
        "    def __init__(self, data, movie_vocab_stoi, user_vocab_stoi):\n",
        "        self.data = data\n",
        "\n",
        "        self.movie_vocab_stoi = movie_vocab_stoi\n",
        "        self.user_vocab_stoi = user_vocab_stoi\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # Fetch data from the dataset\n",
        "    def __getitem__(self, idx):\n",
        "        user, movie_sequence = self.data[idx]\n",
        "        # Directly index into the vocabularies\n",
        "        movie_data = [self.movie_vocab_stoi[item] for item in movie_sequence]\n",
        "        user_data = self.user_vocab_stoi[user]\n",
        "        return torch.tensor(movie_data), torch.tensor(user_data)\n",
        "\n",
        "\n",
        "# Collate function and padding\n",
        "def collate_batch(batch):\n",
        "    movie_list = [item[0] for item in batch]\n",
        "    user_list = [item[1] for item in batch]\n",
        "    return pad_sequence(movie_list, padding_value=movie_vocab_stoi['<unk>'], batch_first=True), torch.stack(user_list)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "# Create instances of your Dataset for each set\n",
        "train_dataset = MovieSeqDataset(train_data_raw, movie_vocab_stoi, user_vocab_stoi)\n",
        "val_dataset = MovieSeqDataset(test_data_raw, movie_vocab_stoi, user_vocab_stoi)\n",
        "# Create DataLoaders\n",
        "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=collate_batch)\n",
        "val_iter = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "YAjJ0nTp_4uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Definition\n",
        "In this section we will define and initialize our model. Then the model will be trained with our previously generated dataset.\n",
        "## 2.1 Positional Encoder\n",
        "We start by defining the positional encoder, which is crucial for sequence-based models like the Transformer. This encoder will capture the positions of movie interactions in our sequences, thus embedding the order information that the Transformer model needs."
      ],
      "metadata": {
        "id": "ZRSuW2NXAkfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "\n",
        "        # `div_term` is used in the calculation of the sinusoidal values.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Initializing positional encoding matrix with zeros.\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "\n",
        "        # Calculating the positional encodings.\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "YIOSMBCtAanG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Transformer Model\n",
        "Following the definition of our positional encoder, we then establish our transformer model. This model takes both the user id and the movie id sequence as input, and it is responsible for generating the output movie predictions."
      ],
      "metadata": {
        "id": "Z4wtPnqQAsOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken: int, nuser: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        # positional encoder\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Multihead attention mechanism.\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "        # Embedding layers\n",
        "        self.movie_embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.user_embedding = nn.Embedding(nuser, d_model)\n",
        "\n",
        "        # Defining the size of the input to the model.\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear layer to map the output tomovie vocabulary.\n",
        "        self.linear = nn.Linear(2*d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        # Initializing the weights of the embedding and linear layers.\n",
        "        initrange = 0.1\n",
        "        self.movie_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.user_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, user: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        # Embedding movie ids and userid\n",
        "        movie_embed = self.movie_embedding(src) * math.sqrt(self.d_model)\n",
        "        user_embed = self.user_embedding(user) * math.sqrt(self.d_model)\n",
        "\n",
        "        # positional encoding\n",
        "        movie_embed = self.pos_encoder(movie_embed)\n",
        "\n",
        "        # generating output with final layers\n",
        "        output = self.transformer_encoder(movie_embed, src_mask)\n",
        "\n",
        "        # Expand user_embed tensor along the sequence length dimension\n",
        "        user_embed = user_embed.expand(-1, output.size(1), -1)\n",
        "\n",
        "        # Concatenate user embeddings with transformer output\n",
        "        output = torch.cat((output, user_embed), dim=-1)\n",
        "\n",
        "        output = self.linear(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Zef8tq8NAo7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the model definitions, we proceed to initialize our model using a set of arbitrarily selected hyperparameters."
      ],
      "metadata": {
        "id": "3M5XCLFG_LLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(movie_vocab)  # size of vocabulary\n",
        "nusers = len(user_vocab)\n",
        "emsize = 128  # embedding dimension\n",
        "d_hid = 128  # dimension of the feedforward network model\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TransformerModel(ntokens, nusers, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 1.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "metadata": {
        "id": "8pEgP1gg-re5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train & Evaluation\n",
        "We're now ready to kick off the training process with our model, where it will learn from the dataset we've prepared. Following the training phase, we'll evaluate how well our model performs on unseen data to check its effectiveness.\n",
        "## 3.1 Train Function"
      ],
      "metadata": {
        "id": "6c8xnSds_Pf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, train_iter, epoch) -> None:\n",
        "    # Switch to training mode\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (movie_data, user_data) in enumerate(train_iter):\n",
        "        # Load movie sequence and user id\n",
        "        movie_data, user_data = movie_data.to(device), user_data.to(device)\n",
        "        user_data = user_data.reshape(-1, 1)\n",
        "\n",
        "        # Split movie sequence to inputs and targets\n",
        "        inputs, targets = movie_data[:, :-1], movie_data[:, 1:]\n",
        "        targets_flat = targets.reshape(-1)\n",
        "\n",
        "        # Predict movies\n",
        "        output = model(inputs, user_data)\n",
        "        output_flat = output.reshape(-1, ntokens)\n",
        "\n",
        "        # Backpropogation process\n",
        "        loss = criterion(output_flat, targets_flat)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        # Results\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "metadata": {
        "id": "gW7vxqTV_Kn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Evaluation Function"
      ],
      "metadata": {
        "id": "VKnPqYte_6si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    # Switch the model to evaluation mode.\n",
        "    # This is necessary for layers like dropout,\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (movie_data, user_data) in enumerate(eval_data):\n",
        "            # Load movie sequence and user id\n",
        "            movie_data, user_data = movie_data.to(device), user_data.to(device)\n",
        "            user_data = user_data.reshape(-1, 1)\n",
        "            # Split movie sequence to inputs and targets\n",
        "            inputs, targets = movie_data[:, :-1], movie_data[:, 1:]\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            # Predict movies\n",
        "            output = model(inputs, user_data)\n",
        "            output_flat = output.reshape(-1, ntokens)\n",
        "            # Calculate loss\n",
        "            loss = criterion(output_flat, targets_flat)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "ex23S0dK_kM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Train & Evaluation Loop"
      ],
      "metadata": {
        "id": "qJNdsU06_2ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 10\n",
        "\n",
        "with TemporaryDirectory() as tempdir:\n",
        "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        train(model, train_iter, epoch)\n",
        "\n",
        "        # Evaluation\n",
        "        val_loss = evaluate(model, val_iter)\n",
        "\n",
        "        # Compute the perplexity of the validation loss\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "\n",
        "        # Results\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGs7vxT0_18W",
        "outputId": "a07ffae2-710a-43ca-9f49-ad3fbd040cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 lr 1.00 | ms/batch 14.73 | loss  7.80 | ppl  2445.23\n",
            "| epoch   1 lr 1.00 | ms/batch 11.26 | loss  7.64 | ppl  2069.84\n",
            "| epoch   1 lr 1.00 | ms/batch 12.87 | loss  7.61 | ppl  2009.38\n",
            "| epoch   1 lr 1.00 | ms/batch 15.05 | loss  7.58 | ppl  1966.59\n",
            "| epoch   1 lr 1.00 | ms/batch 10.70 | loss  7.56 | ppl  1911.28\n",
            "| epoch   1 lr 1.00 | ms/batch 10.64 | loss  7.52 | ppl  1846.37\n",
            "| epoch   1 lr 1.00 | ms/batch 10.74 | loss  7.41 | ppl  1648.97\n",
            "| epoch   1 lr 1.00 | ms/batch 10.67 | loss  7.26 | ppl  1416.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 22.66s | valid loss  7.11 | valid ppl  1222.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 lr 0.95 | ms/batch 18.52 | loss  6.99 | ppl  1086.24\n",
            "| epoch   2 lr 0.95 | ms/batch 12.29 | loss  6.85 | ppl   943.61\n",
            "| epoch   2 lr 0.95 | ms/batch 10.96 | loss  6.78 | ppl   877.27\n",
            "| epoch   2 lr 0.95 | ms/batch 13.00 | loss  6.70 | ppl   815.76\n",
            "| epoch   2 lr 0.95 | ms/batch 14.63 | loss  6.65 | ppl   775.73\n",
            "| epoch   2 lr 0.95 | ms/batch 27.76 | loss  6.59 | ppl   730.18\n",
            "| epoch   2 lr 0.95 | ms/batch 18.92 | loss  6.56 | ppl   706.30\n",
            "| epoch   2 lr 0.95 | ms/batch 10.90 | loss  6.52 | ppl   677.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 28.56s | valid loss  6.53 | valid ppl   688.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 lr 0.90 | ms/batch 13.97 | loss  6.40 | ppl   603.80\n",
            "| epoch   3 lr 0.90 | ms/batch 10.80 | loss  6.36 | ppl   579.40\n",
            "| epoch   3 lr 0.90 | ms/batch 10.51 | loss  6.34 | ppl   565.90\n",
            "| epoch   3 lr 0.90 | ms/batch 10.54 | loss  6.33 | ppl   562.42\n",
            "| epoch   3 lr 0.90 | ms/batch 10.63 | loss  6.32 | ppl   553.22\n",
            "| epoch   3 lr 0.90 | ms/batch 14.73 | loss  6.30 | ppl   542.68\n",
            "| epoch   3 lr 0.90 | ms/batch 12.38 | loss  6.28 | ppl   534.92\n",
            "| epoch   3 lr 0.90 | ms/batch 10.72 | loss  6.28 | ppl   532.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 21.38s | valid loss  6.37 | valid ppl   584.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 lr 0.86 | ms/batch 10.93 | loss  6.17 | ppl   478.78\n",
            "| epoch   4 lr 0.86 | ms/batch 11.81 | loss  6.14 | ppl   464.63\n",
            "| epoch   4 lr 0.86 | ms/batch 14.95 | loss  6.14 | ppl   464.70\n",
            "| epoch   4 lr 0.86 | ms/batch 10.75 | loss  6.14 | ppl   463.34\n",
            "| epoch   4 lr 0.86 | ms/batch 12.12 | loss  6.13 | ppl   458.18\n",
            "| epoch   4 lr 0.86 | ms/batch 13.57 | loss  6.14 | ppl   463.72\n",
            "| epoch   4 lr 0.86 | ms/batch 10.67 | loss  6.13 | ppl   460.42\n",
            "| epoch   4 lr 0.86 | ms/batch 15.27 | loss  6.12 | ppl   454.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 22.80s | valid loss  6.30 | valid ppl   545.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 lr 0.81 | ms/batch 11.25 | loss  6.01 | ppl   408.93\n",
            "| epoch   5 lr 0.81 | ms/batch 10.57 | loss  5.99 | ppl   399.20\n",
            "| epoch   5 lr 0.81 | ms/batch 10.26 | loss  6.01 | ppl   406.77\n",
            "| epoch   5 lr 0.81 | ms/batch 12.04 | loss  6.01 | ppl   405.71\n",
            "| epoch   5 lr 0.81 | ms/batch 14.62 | loss  6.02 | ppl   413.58\n",
            "| epoch   5 lr 0.81 | ms/batch 10.62 | loss  6.02 | ppl   409.73\n",
            "| epoch   5 lr 0.81 | ms/batch 13.63 | loss  6.02 | ppl   412.79\n",
            "| epoch   5 lr 0.81 | ms/batch 10.94 | loss  6.02 | ppl   409.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 21.26s | valid loss  6.26 | valid ppl   523.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 lr 0.77 | ms/batch 16.16 | loss  5.91 | ppl   370.04\n",
            "| epoch   6 lr 0.77 | ms/batch 11.33 | loss  5.90 | ppl   366.65\n",
            "| epoch   6 lr 0.77 | ms/batch 10.48 | loss  5.90 | ppl   365.26\n",
            "| epoch   6 lr 0.77 | ms/batch 10.61 | loss  5.91 | ppl   370.31\n",
            "| epoch   6 lr 0.77 | ms/batch 10.43 | loss  5.92 | ppl   371.83\n",
            "| epoch   6 lr 0.77 | ms/batch 11.34 | loss  5.92 | ppl   372.23\n",
            "| epoch   6 lr 0.77 | ms/batch 15.33 | loss  5.92 | ppl   373.71\n",
            "| epoch   6 lr 0.77 | ms/batch 10.86 | loss  5.93 | ppl   375.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 21.82s | valid loss  6.24 | valid ppl   513.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 lr 0.74 | ms/batch 11.11 | loss  5.82 | ppl   337.87\n",
            "| epoch   7 lr 0.74 | ms/batch 10.42 | loss  5.82 | ppl   336.91\n",
            "| epoch   7 lr 0.74 | ms/batch 13.94 | loss  5.82 | ppl   337.83\n",
            "| epoch   7 lr 0.74 | ms/batch 13.31 | loss  5.83 | ppl   342.00\n",
            "| epoch   7 lr 0.74 | ms/batch 10.75 | loss  5.84 | ppl   345.39\n",
            "| epoch   7 lr 0.74 | ms/batch 10.57 | loss  5.85 | ppl   345.59\n",
            "| epoch   7 lr 0.74 | ms/batch 10.64 | loss  5.86 | ppl   349.26\n",
            "| epoch   7 lr 0.74 | ms/batch 10.64 | loss  5.85 | ppl   346.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 21.96s | valid loss  6.23 | valid ppl   507.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 lr 0.70 | ms/batch 11.07 | loss  5.76 | ppl   316.40\n",
            "| epoch   8 lr 0.70 | ms/batch 10.50 | loss  5.75 | ppl   313.03\n",
            "| epoch   8 lr 0.70 | ms/batch 10.49 | loss  5.76 | ppl   317.23\n",
            "| epoch   8 lr 0.70 | ms/batch 10.53 | loss  5.77 | ppl   319.94\n",
            "| epoch   8 lr 0.70 | ms/batch 11.89 | loss  5.78 | ppl   324.01\n",
            "| epoch   8 lr 0.70 | ms/batch 14.78 | loss  5.78 | ppl   322.87\n",
            "| epoch   8 lr 0.70 | ms/batch 10.85 | loss  5.79 | ppl   326.60\n",
            "| epoch   8 lr 0.70 | ms/batch 10.63 | loss  5.79 | ppl   328.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 20.65s | valid loss  6.22 | valid ppl   505.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 lr 0.66 | ms/batch 11.17 | loss  5.70 | ppl   297.54\n",
            "| epoch   9 lr 0.66 | ms/batch 14.38 | loss  5.68 | ppl   292.14\n",
            "| epoch   9 lr 0.66 | ms/batch 12.78 | loss  5.70 | ppl   298.77\n",
            "| epoch   9 lr 0.66 | ms/batch 10.67 | loss  5.71 | ppl   300.69\n",
            "| epoch   9 lr 0.66 | ms/batch 10.50 | loss  5.72 | ppl   306.35\n",
            "| epoch   9 lr 0.66 | ms/batch 10.57 | loss  5.72 | ppl   304.66\n",
            "| epoch   9 lr 0.66 | ms/batch 10.68 | loss  5.74 | ppl   311.42\n",
            "| epoch   9 lr 0.66 | ms/batch 15.03 | loss  5.75 | ppl   313.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 21.88s | valid loss  6.22 | valid ppl   503.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 lr 0.63 | ms/batch 11.66 | loss  5.64 | ppl   282.06\n",
            "| epoch  10 lr 0.63 | ms/batch 15.50 | loss  5.63 | ppl   278.32\n",
            "| epoch  10 lr 0.63 | ms/batch 10.90 | loss  5.66 | ppl   287.50\n",
            "| epoch  10 lr 0.63 | ms/batch 15.20 | loss  5.66 | ppl   286.18\n",
            "| epoch  10 lr 0.63 | ms/batch 11.77 | loss  5.66 | ppl   288.17\n",
            "| epoch  10 lr 0.63 | ms/batch 10.43 | loss  5.68 | ppl   293.64\n",
            "| epoch  10 lr 0.63 | ms/batch 10.40 | loss  5.69 | ppl   295.04\n",
            "| epoch  10 lr 0.63 | ms/batch 10.68 | loss  5.70 | ppl   300.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 22.04s | valid loss  6.22 | valid ppl   502.60\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Generating Popular Movie Recommendations as Baseline\n",
        "In order to compare our model success a baseline recommendation method is required. One of the easiest recommendation method is popular movie recommendation which is obtained by most frequent and highly rated movies."
      ],
      "metadata": {
        "id": "RDZJozuLjIUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_popular_movies(df_ratings):\n",
        "  # Calculate the number of ratings for each movie\n",
        "  rating_counts = df_ratings['movie_id'].value_counts().reset_index()\n",
        "  rating_counts.columns = ['movie_id', 'rating_count']\n",
        "\n",
        "  # Get the most frequently rated movies\n",
        "  min_ratings_threshold = rating_counts['rating_count'].quantile(0.95)\n",
        "\n",
        "  # Filter movies based on the minimum number of ratings\n",
        "  popular_movies = ratings.merge(rating_counts, on='movie_id')\n",
        "  popular_movies = popular_movies[popular_movies['rating_count'] >= min_ratings_threshold]\n",
        "\n",
        "\n",
        "  # Calculate the average rating for each movie\n",
        "  average_ratings = popular_movies.groupby('movie_id')['rating'].mean().reset_index()\n",
        "  # Get the top 10 rated movies\n",
        "  top_10_movies = list(average_ratings.sort_values('rating', ascending=False).head(10).movie_id.values)\n",
        "  return top_10_movies"
      ],
      "metadata": {
        "id": "_buSM6b5YHuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_movies = get_popular_movies(ratings)\n",
        "[movie_title_dict[movie] for movie in top_10_movies]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDY4JvP-o3xZ",
        "outputId": "b199b393-5344-45d8-eb40-ade777c42339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Shawshank Redemption, The (1994)',\n",
              " 'Godfather, The (1972)',\n",
              " 'Usual Suspects, The (1995)',\n",
              " \"Schindler's List (1993)\",\n",
              " 'Raiders of the Lost Ark (1981)',\n",
              " 'Star Wars: Episode IV - A New Hope (1977)',\n",
              " 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)',\n",
              " 'Casablanca (1942)',\n",
              " 'Sixth Sense, The (1999)',\n",
              " \"One Flew Over the Cuckoo's Nest (1975)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Recommendations Result Comparison\n",
        "Like the evaluation function we will iterate our validation dataset and store recommendation results in lists to compare them with normalized discounted gain(NDCG) metric."
      ],
      "metadata": {
        "id": "z9edNBt-ijnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Movie id decoder\n",
        "movie_vocab_itos = movie_vocab.get_itos()\n",
        "\n",
        "# A placeholders to store results of recommendations\n",
        "transformer_reco_results = list()\n",
        "popular_reco_results = list()\n",
        "\n",
        "# Get top 10 movies\n",
        "k = 10\n",
        "# Iterate over the validation data\n",
        "for i, (movie_data, user_data) in enumerate(val_iter):\n",
        "    # Feed the input and get the outputs\n",
        "    movie_data, user_data = movie_data.to(device), user_data.to(device)\n",
        "    user_data = user_data.reshape(-1, 1)\n",
        "    inputs, targets = movie_data[:, :-1], movie_data[:, 1:]\n",
        "    output = model(inputs, user_data)\n",
        "    output_flat = output.reshape(-1, ntokens)\n",
        "    targets_flat = targets.reshape(-1)\n",
        "\n",
        "    # Reshape the output_flat to get top predictions\n",
        "    outputs = output_flat.reshape(output_flat.shape[0] // inputs.shape[1],\n",
        "                                  inputs.shape[1],\n",
        "                                  output_flat.shape[1])[: , -1, :]\n",
        "    # k + len(inputs) = 13 movies obtained\n",
        "    # In order to prevent to recommend already watched movies\n",
        "    values, indices = outputs.topk(k + inputs.shape[1], dim=-1)\n",
        "\n",
        "    for sub_sequence, sub_indice_org in zip(movie_data, indices):\n",
        "        sub_indice_org = sub_indice_org.cpu().detach().numpy()\n",
        "        sub_sequence = sub_sequence.cpu().detach().numpy()\n",
        "\n",
        "        # Generate mask array to eliminate already watched movies\n",
        "        mask = np.isin(sub_indice_org, sub_sequence[:-1], invert=True)\n",
        "\n",
        "        # After masking get top k movies\n",
        "        sub_indice = sub_indice_org[mask][:k]\n",
        "\n",
        "        # Generate results array\n",
        "        transformer_reco_result = np.isin(sub_indice, sub_sequence[-1]).astype(int)\n",
        "\n",
        "        # Decode movie to search in popular movies\n",
        "        target_movie_decoded = movie_vocab_itos[sub_sequence[-1]]\n",
        "        popular_reco_result = np.isin(top_10_movies, target_movie_decoded).astype(int)\n",
        "\n",
        "        transformer_reco_results.append(transformer_reco_result)\n",
        "        popular_reco_results.append(popular_reco_result)"
      ],
      "metadata": {
        "id": "7XtOtvbCamoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After generating result for each recommendation now time to compare baseline method vs transformer model."
      ],
      "metadata": {
        "id": "lWid7M9fbx7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Since we have already sorted our recommendations\n",
        "# An array that represent our recommendation scores is used.\n",
        "representative_array = [[i for i in range(k, 0, -1)]] * len(transformer_reco_results)\n",
        "\n",
        "for k in [3, 5, 10]:\n",
        "  transformer_result = ndcg_score(transformer_reco_results,\n",
        "                                  representative_array, k=k)\n",
        "  popular_result = ndcg_score(popular_reco_results,\n",
        "                              representative_array, k=k)\n",
        "\n",
        "  print(f\"Transformer NDCG result at top {k}: {round(transformer_result, 4)}\")\n",
        "  print(f\"Popular recommendation NDCG result at top {k}: {round(popular_result, 4)}\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DYVH2JmpHrW",
        "outputId": "87408ed5-d307-41c7-bf3b-a52c772b94e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer NDCG result at top 3: 0.0562\n",
            "Popular recommendation NDCG result at top 3: 0.004\n",
            "\n",
            "\n",
            "Transformer NDCG result at top 5: 0.0709\n",
            "Popular recommendation NDCG result at top 5: 0.0056\n",
            "\n",
            "\n",
            "Transformer NDCG result at top 10: 0.0932\n",
            "Popular recommendation NDCG result at top 10: 0.0086\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have seen our model results are approximately 10 times better than popular movie recommendation at NDCG metric. A function to generate recommendation for single data is given below."
      ],
      "metadata": {
        "id": "3rvyhAueb2f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendation(user_id, movie_sequence, k=10):\n",
        "    model.eval()\n",
        "    input_sequence = movie_sequence[:-1]\n",
        "    # Tokenize and numerically encode the user id and movie sequence\n",
        "    user_tensor = torch.tensor(user_vocab_stoi[user_id])\n",
        "    movie_tensor = torch.tensor([[movie_vocab_stoi[movie_id]] for movie_id in input_sequence])\n",
        "    # Shape: [1, 1]\n",
        "    user_tensor = user_tensor.unsqueeze(0).to(device)\n",
        "    user_tensor = user_tensor.view(user_tensor.shape[0], 1)\n",
        "\n",
        "    # Shape: [1, seq_length]\n",
        "    movie_tensor = movie_tensor.unsqueeze(0).to(device)[0]\n",
        "    movie_tensor = movie_tensor.view(1, movie_tensor.shape[0])\n",
        "\n",
        "    # Pass the tensors through the model\n",
        "    with torch.no_grad():\n",
        "        predictions = model(movie_tensor, user_tensor)\n",
        "\n",
        "    # The output is a probability distribution over the next movie.\n",
        "    # Topk to get most probable movies\n",
        "    values, indices = predictions.topk(k + len(input_sequence), dim=-1)\n",
        "    # Eliminate already watched movies\n",
        "    indices = [indice for indice in indices[-1, :][0] if indice not in movie_tensor][:k]\n",
        "    predicted_movies = [movie_title_dict[movie_vocab.get_itos()[movie]] for movie in indices]\n",
        "    return predicted_movies"
      ],
      "metadata": {
        "id": "XAyDjSxV_pZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_iter = test_data_raw[59233]\n",
        "print(\"Input Sequence:\")\n",
        "print(\"-\" + \"\\n-\".join([movie_title_dict[ea_movie] for ea_movie in row_iter[1][:-1]]))\n",
        "recos = '\\n-'.join(generate_recommendation(row_iter[0],row_iter[1]))\n",
        "\n",
        "print(f\"Recomendations:\\n-{recos}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcuSBAP5fGXb",
        "outputId": "0e4433d7-4c52-4fc1-b262-0bfdadca4be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence:\n",
            "-Ace Ventura: Pet Detective (1994)\n",
            "-Home Alone (1990)\n",
            "-Tommy Boy (1995)\n",
            "Recomendations:\n",
            "-Dumb & Dumber (1994)\n",
            "-Adventures in Babysitting (1987)\n",
            "-Fletch Lives (1989)\n",
            "-White Men Can't Jump (1992)\n",
            "-Abyss, The (1989)\n",
            "-Ace Ventura: When Nature Calls (1995)\n",
            "-Benny & Joon (1993)\n",
            "-Lethal Weapon 2 (1989)\n",
            "-Addams Family, The (1991)\n",
            "-Ghost (1990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "In this blog post, we have made an attempt to use the Transformer model, known for its effectiveness in NLP, to create a personalized movie recommendation system. We've gone through from data preprocessing to prediction step using the MovieLens dataset. While this is a starting point and there's much more to learn, it hopefully sheds some light on how Transformer models can be used in different contexts, such as recommendation systems.\n",
        "\n",
        "# References\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "https://keras.io/examples/structured_data/movielens_recommendations_transformers/\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html"
      ],
      "metadata": {
        "id": "XO1cop5EcU52"
      }
    }
  ]
}